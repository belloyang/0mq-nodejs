#include "ReadAheadIOContext.h"

#if !defined(DEV_PLATFORM_STM32)

#include <cstring>
#include <cinttypes> // PRIu64
#include <algorithm>

#include <stdexcept>

ReadAheadIOContext::ReadAheadIOContext(const std::shared_ptr<IIOContext>& baseContext,
    const std::shared_ptr<ILogger>& logger)
: IIOContext(logger)
, m_baseContext(baseContext)
, m_totalSize(0)
, m_bFinishRequested(false)
, m_bFinished(false)
{
  __DEV_CALLSTACK_FUNC__;
  __ASSERT__(baseContext != nullptr && "'baseContext' cannot be nullptr here");
  ::memset(&m_state, 0, sizeof(m_state));
  //tss_harv::start read ahead thread
  {
    std::thread readAheadThread(&ReadAheadIOContext::_readAheadThread, this);
    readAheadThread.detach();
  }
  m_totalSize = m_baseContext->getSize();
}

ReadAheadIOContext::~ReadAheadIOContext()
{
  __DEV_CALLSTACK_FUNC__;
  // request finish
  m_bFinishRequested = true;
  // wake up _readAheadThread
  m_readAheadEvent.notify_one();
  // wait till read ahead thread finishes
  std::unique_lock<std::mutex> lk(m_mutex);
  while (!m_bFinished)
    m_readAheadThreadFinished.wait(lk);
  // cleanup buffer data
  if (m_state.completed_buffer.data != nullptr)
    ::free(m_state.completed_buffer.data);
  if (m_state.pending_buffer.data != nullptr)
    ::free(m_state.pending_buffer.data);  
}

size_t ReadAheadIOContext::_checkBlock(io_offset_t offset, size_t blockSize) const
{
  __DEV_CALLSTACK_FUNC__;
  if (offset >= m_totalSize)
  {
    m_logger->error(__THIS_FUNC__, eErrorCategory::ERROR_CATEGORY_LIBRARY, eLibIOErrorCode::LIBIO_ERROR_IO_INVALID_RANGE,
      "Out of range [0, %" PRIu64 "] access detected (offset=%" PRIu64 ")",
      m_totalSize - 1LLU,
      offset);
    return 0;
  }
  if (offset + blockSize > m_totalSize)
  {
    const size_t availableBlockSize = (size_t)std::min<uint64_t>(blockSize, m_totalSize - offset);
    m_logger->warning(__THIS_FUNC__, "Partial out of range [0, %" PRIu64 "] access detected [%" PRIu64 ", %" PRIu64 "]. Block size (%lu) reduced to %lu",
      m_totalSize - 1LLU,
      offset, offset + blockSize - 1LLU,
      (unsigned long)blockSize, (unsigned long)availableBlockSize);
    return availableBlockSize;
  }

  return blockSize;
}

void ReadAheadIOContext::_readAheadThread()
{
  __DEV_CALLSTACK_FUNC__;

  try
  {
    while (!m_bFinishRequested)
    {
      std::unique_lock<std::mutex> lk(m_mutex);
      // wait for new read request (implies spurious wake-up check) or finish request
      while (!m_bFinishRequested &&
        ((m_state.requested.offset == m_state.actual.offset) && // offset data hasn't changed
        (m_state.requested.size <= m_state.actual.size))) // smaller (not bigger) data requested at the same offset -> keep waiting
      {
        m_readAheadEvent.wait(lk);
      }
      if (m_bFinishRequested) {
        break;
      }

      // determine number of bytes to read
      const size_t nBytesToRead = this->_checkBlock(m_state.requested.offset, m_state.requested.size);
      if (nBytesToRead <= 0) {
        continue;
      }

      // prepare for IO operation
      m_state.ioStatus = sReadAheadState::IO_PENDING;
      // extend pending buffer if needed
      if (m_state.pending_buffer.size < nBytesToRead)
      {
        m_state.pending_buffer.size = nBytesToRead;
        m_state.pending_buffer.data = ::realloc(m_state.pending_buffer.data, nBytesToRead);
      }
      // create the copy for state and release mutex
      sReadAheadState stateCopy = m_state; // shallow copy
      lk.unlock();

      // read data into pending buffer
      size_t readSize = 0;
      {
        std::lock_guard<std::mutex> ioLock(m_ioMutex);
        readSize = m_baseContext->readBlock(stateCopy.requested.offset, nBytesToRead, stateCopy.pending_buffer.data);
      }

      // IO operation finished; switch pending buffer with active buffer or handle IO error
      lk.lock();
      if (readSize > 0)
      {
        m_state.ioStatus = sReadAheadState::IO_READY;
        m_state.actual.offset = stateCopy.requested.offset;
        m_state.actual.size = readSize;
        m_state.pending_buffer = m_state.completed_buffer; // make the completed buffer pending
        m_state.completed_buffer = stateCopy.pending_buffer; // make the pending buffer completed
      }
      else
      {
        // once an IO error occurs prevent any further IO
        m_state.ioStatus = sReadAheadState::IO_ERROR;
        m_bFinishRequested = true;
      }
      lk.unlock();

      // raise ahead-data-read event
      m_aheadDataRead.notify_all();
    }
  }
  catch (const std::exception& e)
  {
    m_logger->error(__THIS_FUNC__, eErrorCategory::ERROR_CATEGORY_LIBRARY, eLibIOErrorCode::LIBIO_ERROR_IO_READING_FAILED,
      "Exception caught: '%s'", e.what());
  }
  catch (...)
  {
    m_logger->error(__THIS_FUNC__, eErrorCategory::ERROR_CATEGORY_LIBRARY, eLibIOErrorCode::LIBIO_ERROR_IO_READING_FAILED,
      "Exception caught: UNKNOWN");
  }

  // mark the thread as finished and raise an event
  m_bFinished = true;
  m_readAheadThreadFinished.notify_all();
}

void ReadAheadIOContext::_readAhead(const io_offset_t offset, const size_t blockSize)
{
  __DEV_CALLSTACK_FUNC__;
  if (offset >= m_totalSize)
  {
    m_readAheadEvent.notify_one();
    return;
  }

  // calculate block size available for reading
  const size_t availableBlockSize = static_cast<size_t>(std::min<uint64_t>(blockSize, m_totalSize - offset));
  {
    std::lock_guard<std::mutex> lockGuard(m_mutex);
    m_state.requested.offset = offset;
    m_state.requested.size = availableBlockSize;
  }
  //tss_harv::wake up _readAheadThread
  m_readAheadEvent.notify_one();
}

size_t ReadAheadIOContext::readBlock(io_offset_t offset, size_t blockSize, void* pOutput)
{
  __DEV_CALLSTACK_FUNC__;
  const size_t blockSizeToRead = this->_checkBlock(offset, blockSize);
  if (blockSizeToRead <= 0)
    return 0;

  size_t bytesRead = 0;
  {
    std::unique_lock<std::mutex> lock(m_mutex);
    // if pending data covers requested -> wait for _readAheadThread to produce the data
    if ( (m_state.requested.offset <= offset) && 
         (m_state.requested.offset + m_state.requested.size >= offset + blockSizeToRead))
    {
      while (m_state.ioStatus == sReadAheadState::IO_PENDING)
      {
        m_aheadDataRead.wait(lock);
      }
    }

    // abort if an IO error occurred
    if (m_state.ioStatus == sReadAheadState::IO_ERROR) {
      return 0;
    }
    
    // check if we need to read the block that was already read
    if ( (m_state.actual.offset <= offset) && 
         (m_state.actual.offset + m_state.actual.size) >= (offset + blockSizeToRead))
    {
      //m_logger->debug(__THIS_FUNC__, "Data read ahead is being reused/copied");
      const io_offset_t inBlockOffset = (offset - m_state.actual.offset);
      ::memcpy(pOutput, (char*)m_state.completed_buffer.data + inBlockOffset, blockSizeToRead);
      lock.unlock();
      bytesRead = blockSizeToRead;
    }
    else // direct blocking read operation call
    {
      // release m_state mutex
      lock.unlock();
      // protected I/O operation
      {
        std::lock_guard<std::mutex> ioLock(m_ioMutex);
        bytesRead = m_baseContext->readBlock(offset, blockSizeToRead, pOutput);
      }
    }
  }
  
  // schedule next read ahead
  this->_readAhead(offset + blockSize, blockSize);
  return bytesRead;
}

size_t ReadAheadIOContext::writeBlock(io_offset_t offset, size_t blockSize, const void* pData)
{
  __DEV_CALLSTACK_FUNC__;
  std::lock_guard<std::mutex> lock(m_ioMutex);
  return m_baseContext->writeBlock(offset, blockSize, pData);
}

void ReadAheadIOContext::flush()
{
  __DEV_CALLSTACK_FUNC__;
  std::lock_guard<std::mutex> lock(m_ioMutex);
  m_baseContext->flush();
}

uint64_t ReadAheadIOContext::getSize() const
{
  __DEV_CALLSTACK_FUNC__;
  return m_totalSize;
}

#endif //!defined(DEV_PLATFORM_STM32)
