#include "PacketSlicer_TimeRange.h"

#include "../../steim/decoders/SteimDecoder_Steim1.h"
#include "../../steim/decoders/SteimDecoder_Steim2.h"
#include "../../steim/synthesizers/MiniSEEDRecordsSynthesizer_Steim1.h"
#include "../../steim/synthesizers/MiniSEEDRecordsSynthesizer_Steim2.h"
#include "../../Utils/harvester_utils.h"

#include <Utils/endianness_utils.h>

#include <libmseed.h>

#include <algorithm>

#define LOG_SLICES_INFO (false)

class Steim2Synthesizer_Delegate : public IMiniSEEDRecordsSynthesizerDelegate
{
public:
  Steim2Synthesizer_Delegate(std::vector<MiniSEEDRecordData>& output,
    const std::shared_ptr<ILogger>& logger)
    : IMiniSEEDRecordsSynthesizerDelegate(logger)
    , m_output(output)
  {
    __DEV_CALLSTACK_FUNC__;
  }

  virtual void onRecordReady(const MiniSEEDRecordData& recordData) __OVERRIDES__(IMiniSEEDRecordsSynthesizerDelegate)
  {
    __DEV_CALLSTACK_FUNC__;
    m_output.emplace_back(recordData);
  }

private:
  std::vector<MiniSEEDRecordData>& m_output;
};

PacketSlicer_TimeRange::PacketSlicer_TimeRange(const sTimespan& timeRange, const sSequenceRange& sequenceRange,
  const timediff_t sliceLengthNs, const std::shared_ptr<ILogger>& logger)
: m_logger(logger)
, m_timeRange(timeRange)
, m_sequenceRange(sequenceRange)
, m_sliceLengthNs(sliceLengthNs)
, m_decodersMap {
    { PDB_eMediaType_t::MEDIA_STEIM1, std::make_shared<SteimDecoder_Steim1>(logger) },
    { PDB_eMediaType_t::MEDIA_STEIM2, std::make_shared<SteimDecoder_Steim2>(logger) }
  }
, m_synthesizersMap {
    { PDB_eMediaType_t::MEDIA_STEIM1, std::make_shared<MiniSEEDRecordsSynthesizer_Steim1>(nullptr, //do not set the buffer
          std::make_shared<Steim2Synthesizer_Delegate>(m_recompressedRecordsQueue, logger), true, logger) },
    { PDB_eMediaType_t::MEDIA_STEIM2, std::make_shared<MiniSEEDRecordsSynthesizer_Steim2>(nullptr, //do not set the buffer
          std::make_shared<Steim2Synthesizer_Delegate>(m_recompressedRecordsQueue, logger), true, logger) }
  }
, m_samplesBuffer(nullptr)
{
  __DEV_CALLSTACK_FUNC__;
  ::memset(m_lastSamples, 0, sizeof(m_lastSamples));
  //tss_harvlib::create steim frame buffers for 2 records that can be held simultaneously:
  //1 packet max duration is ~108 minutes, min mseeed file duration = 1h -> max splits needed = 2
  m_frameBuffers.push_back(std::make_shared<buffer_malloc<SteimFrame_t>>(PDB_SENSOR_INPUT_FRAMES_COUNT));
  m_frameBuffers.push_back(std::make_shared<buffer_malloc<SteimFrame_t>>(PDB_SENSOR_INPUT_FRAMES_COUNT));
}

PacketSlicer_TimeRange::~PacketSlicer_TimeRange()
{
  __DEV_CALLSTACK_FUNC__;
  //free up samples buffer
  ::free(m_samplesBuffer);
}

std::vector<sPacketSlice> PacketSlicer_TimeRange::slice(const MsgSensorInput* pPacket, const sequence_number_t sequenceNumber)
{
  __DEV_CALLSTACK_FUNC__;
  std::vector<sPacketSlice> output;
  // some debug checks
  __ASSERT__(pPacket != nullptr && "packet cannot be nullptr here");
  const MsgSensorInput::Header* const pHeader = pPacket->getHeader();
  // empty packet check
  if (pHeader->n_samples == 0u)
  {
    return output;
  }
  // retrieve packet information
  const bool isBigEndian = ((pHeader->flags & MsgSensorInput::Header::eFlags::FLAG_BIG_ENDIAN) != 0);
  const double sampleRate = pdb_utils::decode_sample_rate(pHeader->sample_rate);
  const double sampleDurationNsFloat = 1e9 / sampleRate;
  __TO_DO__("Consider accuracy of floating point operations with 'sampleDurationNsFloat'");
  const timestamp_t packetStart = pHeader->timestamp;
  const timestamp_t packetEnd = pHeader->timestamp + static_cast<timediff_t>(sampleDurationNsFloat * pHeader->n_samples); // rely on truncation to fixed point
  const timestamp_t packetLowerBound = (sequenceNumber == m_sequenceRange.lower) ?
      std::max<timestamp_t>(m_timeRange.lower, packetStart) :
      packetStart; // cut off the very first packet by lower time bound (inclusive)
  const timestamp_t packetUpperBound = (sequenceNumber == m_sequenceRange.upper) ?
      std::min<timestamp_t>(packetEnd, m_timeRange.upper) :
      packetEnd; // cut off the very last packet by upper time bound (exclusive)
  /* All the intermediate packets go the miniSEED files completely (probably sliced by hour boundaries, but ALL the samples covered by those slices).
     When we specify sequence range, we mean to harvest all the data in [range.lower.sequence + 1; range.upper.sequence - 1] completely and check and perform slicing by
     time boundaries [range.lower.time; range.upper.time] for "frontier/edge" packets only
     VolumeRefIndex for each event that has it contains the positions of 'write-cursors' (the position/sequence_number for the next element that will be written).
     At the exact moment when event arrives to PDB, 'seismic packet/soh packet/etc' can be 'half-baked', so the clients needs to try to harvest its portion as well
     Thereby, we need to check if boundary packet stays within requested [lower.time, upper.time] and cut it of completely if needed. It's equal the following check:
  */
  if (packetLowerBound >= packetUpperBound)
  {
    return output;
  }

  //tss_harvlib::calculate slice start boundary
  const int32_t startSliceIndex = harvester_utils::get_slice_index(packetStart, m_sliceLengthNs);
  const timestamp_t sliceLowerBound = startSliceIndex * m_sliceLengthNs;
  __ASSERT__(sliceLowerBound <= packetLowerBound && "lower slice boundary should be not bigger than packet lower bound (the first slicing point) "
    "as the slice boundary is calculated for packet start time");

  std::vector<timestamp_t> sliceTimestamps;
  sliceTimestamps.reserve(4); //reserve data for the most common size
  //tss_harvlib::push the very first timestamp into slices
  sliceTimestamps.emplace_back(packetLowerBound); //packet lower bound - the very first slice point

  //tss_harvlib::detect "slicing points" to slice packet
  {
    timestamp_t sliceStart = sliceLowerBound;
    //tss_harvlib::keep slicing till any of packetEnd or m_upperTime reached
    while (sliceStart < packetUpperBound)
    {
      //tss_harv::truncate slice time with respect to slice length and and slicing end time (slicing boundary is exclsuive, "boundary" sample should go to the next slice)
      sliceStart = std::min<timestamp_t>(sliceStart + m_sliceLengthNs, packetUpperBound);
      sliceTimestamps.emplace_back(sliceStart);
    }
  }
  const uint32_t nSlicePoints = static_cast<uint32_t>(sliceTimestamps.size());

  // check if slice matches packet boundaries -> packet fits entirely within slicing bounds
  if (sliceTimestamps[0] == packetStart && sliceTimestamps[1] == packetEnd)
  {
    // This check does assure that we do not get any extra slicing point if the packet has to be slice by its boundaries (not sliced effectively)
    __ASSERT__(nSlicePoints == 2u && "Check the logic. When slicing matches packet times -> should be only 2 slice points!");
    // build slice that contains the entire packet data
    sPacketSlice slice;
    {
      slice.sequenceNumber = sequenceNumber;
      slice.channelIndex = pHeader->input;
      slice.sliceIndex = startSliceIndex;
      slice.firstSampleIndex = 0;
      slice.timestamp = packetStart;
      slice.sampleRate = pHeader->sample_rate;
      slice.mediaType = pHeader->media_type;
      slice.nSamples = pHeader->n_samples;
      slice.data = reinterpret_cast<const char*>(pHeader->frames);
      slice.nFramesCompressed = NUM_ELEMENTS(pHeader->frames);
      slice.dataSizeInBytes = sizeof(pHeader->frames);
      slice.isBigEndian = isBigEndian;
    }
    __ASSERT__(this->_checkSlice(slice, sampleDurationNsFloat) && "slice spills over slicing boundaries");
    output.emplace_back(slice);
  }
  else
  {
    __ASSERT__(nSlicePoints >= 2u && "Check the logic. When slicing should happen -> we should have at least 2 slice points!");
    // calculate sample index (if slicing in between samples -> start from the right one)
    uint32_t firstSampleIndex = (sliceTimestamps[0] == packetStart) ?
        0 : // start from the very first sample
        (uint32_t)((sliceTimestamps[0] - packetStart - 1LL) / sampleDurationNsFloat) + 1U; // rely on truncation to fixed point
    int32_t sliceIndex = startSliceIndex;
    // check if packet cut off completely -> return empty slices array/vector
    if (firstSampleIndex >= pHeader->n_samples)
    {
      return output;
    }

    // decode/uncompress data
    {
      const auto decoderIt = m_decodersMap.find(pHeader->media_type);
      __ASSERT__(decoderIt != m_decodersMap.end() && "decoder for media type wasn't found");
      const std::shared_ptr<ISteimDecoder>& decoder = decoderIt->second;
      // Decode record (steim frames -> to little-endian raw samples) for further re-slicing
      const uint32_t nSamples = decoder->decodeRecord(pHeader->frames, NUM_ELEMENTS(pHeader->frames), isBigEndian, &m_samplesBuffer);
      __ASSERT__(nSamples == pHeader->n_samples && "Sanity check! Steim decoding failed!");
    }
    //tss_harvlib::do packet slicing (decompressed data - INT32)
    for (size_t i = 1; i < nSlicePoints; i++, sliceIndex++)
    {
      sPacketSlice slice;
      {
        slice.sequenceNumber = sequenceNumber;
        slice.timestamp = packetStart + static_cast<timediff_t>(firstSampleIndex * sampleDurationNsFloat); // WARNING: truncation to fixed point
        slice.channelIndex = pHeader->input;
        slice.sliceIndex = sliceIndex;
        slice.sampleRate = pHeader->sample_rate;
        slice.mediaType = PDB_eMediaType_t::MEDIA_UNCOMPRESSED;
        slice.firstSampleIndex = firstSampleIndex;
        //calculate amount of samples covered by slice
        {
          const timediff_t alignedSliceDurationNs = sliceTimestamps[i] - slice.timestamp; // WARNING: 'slice.timestamp' was truncated
          // calculate amount of samples that start inside slice
          slice.nSamples = static_cast<uint32_t>(alignedSliceDurationNs / sampleDurationNsFloat); // rely on truncation to fixed point
        }
        slice.data = reinterpret_cast<const char*>(m_samplesBuffer + slice.firstSampleIndex); // raw/uncompressed sample
        slice.nFramesCompressed = 0u; // raw samples slice has no compressed frames
        slice.dataSizeInBytes = slice.nSamples * sizeof(sample_t);
        slice.isBigEndian = false; // data has been decoded to little-endian during decompression
      }
      if (slice.nSamples > 0u)
      {
        // recompress slice back to initial media type
        this->_recompressSlice(i - 1, slice, pHeader->media_type);
        __ASSERT__(this->_checkSlice(slice, sampleDurationNsFloat) && "slice spills over slicing boundaries");
        // push recompressed slice
        output.emplace_back(slice);
      }
      // increase first sample index for the next slice with respect to the current
      firstSampleIndex += slice.nSamples;
    }
  }

#if (LOG_SLICES_INFO)
{
  uint32_t idx = 0u;
  uint32_t nSamplesInSlices = 0u;
  for (const sPacketSlice& slice : output)
  {
    nSamplesInSlices += slice.nSamples;
    const timestamp_t sliceStart = slice.timestamp;
    static char startBuff[64] = {0};
    const hptime_t startTime = pdb_utils::nanoseconds_to_microseconds(sliceStart);
    ms_hptime2seedtimestr(startTime, startBuff, 1);

    const timestamp_t sliceEnd = slice.timestamp + static_cast<timediff_t>(slice.nSamples * sampleDurationNsFloat);
    const hptime_t endTime = pdb_utils::nanoseconds_to_microseconds(sliceEnd);
    static char endBuff[64] = {0};
    ms_hptime2seedtimestr(endTime, endBuff, 1);
    m_logger->info(__THIS_FUNC__, "Slice #%u: [%5u, %5u] -> %s - %s)", idx++,
      slice.firstSampleIndex, slice.firstSampleIndex + slice.nSamples - 1,
      startBuff, endBuff);
  }
  m_logger->info(__THIS_FUNC__, "Packet (seq #%u): sliced %u -> %u samples", sequenceNumber,
    pPacket->getHeader()->n_samples, nSamplesInSlices);
  // NOTE!!!!!: this assert for harvest ALL only, when no first & last packets are truncated
  //__ASSERT__(pPacket->getHeader()->n_samples == nSamplesInSlices && "Problem here!!!");
}
#endif

  //save the last processed sample in the channel
  const sample_t lastSample = (sample_t)(reinterpret_cast<const steim::SteimFrameData*>(pHeader->frames)[0].datasets1.last_sample);
  m_lastSamples[pHeader->input] = isBigEndian ? endianness_utils::swap(lastSample) : lastSample;

  return output;
}

bool PacketSlicer_TimeRange::_checkSlice(const sPacketSlice& slice, const double sampleDurationNs) const
{
  __DEV_CALLSTACK_FUNC__;
  const timestamp_t startTime = slice.timestamp;
  const timestamp_t endTime = slice.timestamp + static_cast<timediff_t>(slice.nSamples * sampleDurationNs); // upper is exclusive
  // the entire packet should be inside the same global slice index
  const uint32_t startSliceIndex = harvester_utils::get_slice_index(startTime, m_sliceLengthNs);
  const uint32_t endSliceIndex = harvester_utils::get_slice_index(endTime - 1LL, m_sliceLengthNs); // upper is exclusive
#if (0) // enable this to check Steim-1/2 compression/decompression correctness
  // decompress records to perform integrity check
  {
    const SteimFrame_t* frames = reinterpret_cast<const SteimFrame_t*>(slice.data);
    sample_t* samples = nullptr;
    const auto& decoderIt = m_decodersMap.find(slice.mediaType);
    // decodeRecord does integrity check internally
    const uint32_t nSamples = decoderIt->second->decodeRecord(frames, slice.nFramesCompressed,
        slice.isBigEndian, &samples);
    // check samples count match
    __ASSERT__((nSamples == slice.nSamples) && "Compression check failed");
  }
#endif
  return (startSliceIndex == endSliceIndex);
}

void PacketSlicer_TimeRange::_recompressSlice(const uint32_t sliceIndex, sPacketSlice& slice, const PDB_eMediaType_t mediaType)
{
  __DEV_CALLSTACK_FUNC__;
  //tss_harvlib::free up the queue
  m_recompressedRecordsQueue.clear();

  __ASSERT__(slice.nSamples > 0 && slice.dataSizeInBytes > 0 && slice.data != nullptr && "invalid slice supplied!");
  const sample_t* pSamples = reinterpret_cast<const sample_t*>(slice.data);
  const auto& synthesizerIt = m_synthesizersMap.find(mediaType);
  __ASSERT__(synthesizerIt != m_synthesizersMap.end() && "synthesizer for media type is not found!");
  const sample_t firstSample = synthesizerIt->second->normalizeSample(pSamples[0]); __UNUSED__(firstSample);
  const sample_t lastSample = synthesizerIt->second->normalizeSample(pSamples[slice.nSamples - 1]); __UNUSED__(lastSample);
  //tss_harvlib::create Steim2 packet
  {
    const std::shared_ptr<IMiniSEEDRecordsSynthesizer>& synthesizer = synthesizerIt->second;
    //reset frames buffer for slice (each slice has it's own records buffer
    synthesizer->setFramesBuffer(m_frameBuffers[sliceIndex]);
    synthesizer->reset(m_lastSamples[slice.channelIndex]); //reset synthesizer state with last sample to calculate differences starting from
    //build steim2 packet
    for (uint32_t sampleIdx = 0; sampleIdx < slice.nSamples; sampleIdx++)
    {
      const sample_t sample = pSamples[sampleIdx];
      synthesizer->addSample(sample);
    }
    //tss_harvlib::finalize record creation forcibly (even frames buffer is not filled up)
    synthesizer->flush();
  }

  //tss_harvlib::process recompressed samples
  __ASSERT__(m_recompressedRecordsQueue.size() == 1 && "1 record has been unpacked and sliced -> each slice should be recompressed back to 1 record as well");
  const MiniSEEDRecordData& recordData = m_recompressedRecordsQueue.front();

  //tss_harvlib::check recompressed data
  {
    __ASSERT__(recordData.data == m_frameBuffers[sliceIndex]->ptr<const char>() && "Sanity check failed. Invalid buffer received!");
    __ASSERT__(recordData.nSamples == slice.nSamples && "recompression failed!!!! Samples count doesn't match");
    const steim::SteimFrameData* pFrames = reinterpret_cast<const steim::SteimFrameData*>(recordData.data);
    __ASSERT__(((recordData.isBigEndian &&
                  endianness_utils::swap<sample_t>(pFrames[0].datasets1.first_sample) == firstSample &&
                  endianness_utils::swap<sample_t>(pFrames[0].datasets1.last_sample) == lastSample) ||
               (!recordData.isBigEndian &&
                   (sample_t)pFrames[0].datasets1.first_sample == firstSample &&
                   (sample_t)pFrames[0].datasets1.last_sample  == lastSample))
               && "recompression integrity check failed!!!");
  }

  //tss_harvlib::update last sample processed on the channel
  m_lastSamples[slice.channelIndex] = lastSample;

  //tss_harvlib::update slice
  {
    slice.mediaType = recordData.mediaType;
    slice.nSamples = recordData.nSamples;
    slice.nFramesCompressed = recordData.nFrames;
    slice.data = recordData.data;
    slice.dataSizeInBytes = recordData.dataSizeInBytes;
    slice.isBigEndian = recordData.isBigEndian;
  }
}
